{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop = ['002', '003', '011', '014', '022', '023', '039', '048', '049', '050', '051','052', '053']\n",
    "\n",
    "rename_col = {\n",
    "'001' : 'PPL1Y',\n",
    "'004' : 'Age_18T24',\n",
    "'005' : 'Age_25T34',\n",
    "'006' : 'Age_35T44',\n",
    "'007' : 'Age_45T54',\n",
    "'008' : 'Age_55T64',\n",
    "'009' : 'Age_65T74',\n",
    "'010' : 'Age_75T',\n",
    "'012' : 'Gender_Male',\n",
    "'013' : 'Gender_Female',\n",
    "'015' : 'Race_White',\n",
    "'016' : 'Race_Black',\n",
    "'017' : 'Race_AI_AN',\n",
    "'018' : 'Race_Asian',\n",
    "'019' : 'Race_NH_OPI',\n",
    "'020' : 'Race_Other',\n",
    "'021' : 'Race_Mixed',\n",
    "'024' : 'Citizen_Native',\n",
    "'025' : 'Citizen_Foreign_Born',\n",
    "'026' : 'Citizen_Naturalized',\n",
    "'027' : 'Citizen_NotUS',\n",
    "'034' : 'Edu_Less_High',\n",
    "'035' : 'Edu_High_Grad',\n",
    "'036' : 'Edu_Some_College',\n",
    "'037' : 'Edu_Bachelor',\n",
    "'038' : 'Edu_Grad_Prof',\n",
    "'040' : 'Income_1k',\n",
    "'041' : 'Income_15k',\n",
    "'042' : 'Income_25k',\n",
    "'043' : 'Income_35k',\n",
    "'044' : 'Income_50k',\n",
    "'045' : 'Income_65k',\n",
    "'046' : 'Income_75k',\n",
    "'047' : 'Income_75k+',\n",
    "'054' : 'Housing_Owner',\n",
    "'055' : 'Housing_Rental',\n",
    "}\n",
    "\n",
    "transform_col = {\n",
    "    'PPL1Y': ['PPL1Y'], \n",
    "    'AGE_LOW': ['Age_18T24', 'Age_25T34'],\n",
    "    'AGE_MID': ['Age_35T44', 'Age_45T54', 'Age_55T64'],\n",
    "    'AGE_HIGH': ['Age_65T74'],\n",
    "    'GENDER_0': ['Gender_Male'],\n",
    "    'GENDER_1': ['Gender_Female'], \n",
    "    'RACE_0': ['Race_White'],\n",
    "    'RACE_1': ['Race_Black', 'Race_AI_AN', 'Race_Asian', 'Race_NH_OPI', 'Race_Other', 'Race_Mixed'], \n",
    "    'CITIZENSHIP_0': ['Citizen_Native', 'Citizen_Foreign_Born', 'Citizen_Naturalized'], \n",
    "    'CITIZENSHIP_1': ['Citizen_NotUS'],\n",
    "    'EDU_LOW': ['Edu_Less_High', 'Edu_High_Grad'], \n",
    "    'EDU_MID': ['Edu_Some_College', 'Edu_Bachelor'], \n",
    "    'EDU_HIGH': ['Edu_Grad_Prof'], \n",
    "    'INCOME_LOW': ['Income_1k', 'Income_15k', 'Income_25k', 'Income_35k'], \n",
    "    'INCOME_MID': ['Income_50k', 'Income_65k', 'Income_75k'],\n",
    "    'INCOME_HIGH': ['Income_75k+'], \n",
    "    'HOUSE_0': ['Housing_Owner'],\n",
    "    'HOUSE_1': ['Housing_Rental']\n",
    "}\n",
    "\n",
    "top15_cities = {\n",
    "    'New York-Newark-Jersey City, NY-NJ-PA': 35620, \n",
    "    'Los Angeles-Long Beach-Anaheim, CA': 31080,\n",
    "    'Chicago-Naperville-Elgin, IL-IN-WI': 16980,\n",
    "    'Dallas-Fort Worth-Arlington, TX': 19100,\n",
    "    'Houston-The Woodlands-Sugar Land, TX': 26420,\n",
    "    'Washington-Arlington-Alexandria, DC-VA-MD-WV': 47900,\n",
    "    'Miami-Fort Lauderdale-Pompano Beach, FL': 33100,\n",
    "    'Philadelphia-Camden-Wilmington, PA-NJ-DE-MD': 37980,\n",
    "    'Atlanta-Sandy Springs-Alpharetta, GA': 12060,\n",
    "    'Phoenix-Mesa-Chandler, AZ': 38060,\n",
    "    'Boston-Cambridge-Newton, MA-NH': 14460,\n",
    "    'San Francisco-Oakland-Berkeley, CA': 41860,\n",
    "    'Riverside-San Bernardino-Ontario, CA': 40140,\n",
    "    'Detroit-Warren-Dearborn, MI': 19820,\n",
    "    'Seattle-Tacoma-Bellevue, WA': 42660\n",
    "    }\n",
    "\n",
    "total_ppl = {f'S0701_C01_{key}E': f'TT_{val}' for key, val in rename_col.items()}\n",
    "within_county = {f'S0701_C02_{key}E': f'SC_{val}' for key, val in rename_col.items()}\n",
    "within_state = {f'S0701_C03_{key}E': f'SS_{val}' for key, val in rename_col.items()}\n",
    "diff_state = {f'S0701_C04_{key}E': f'DS_{val}' for key, val in rename_col.items()}\n",
    "\n",
    "all_column = total_ppl | within_county | within_state | diff_state\n",
    "\n",
    "total_ppl = {f'TT_{key}': list(map(lambda x: 'TT_' + x, val)) for key, val in transform_col.items()}\n",
    "within_county = {f'SC_{key}': list(map(lambda x: 'SC_' + x, val)) for key, val in transform_col.items()}\n",
    "within_state = {f'SS_{key}': list(map(lambda x: 'SS_' + x, val)) for key, val in transform_col.items()}\n",
    "diff_state = {f'DS_{key}': list(map(lambda x: 'DS_' + x, val)) for key, val in transform_col.items()}\n",
    "\n",
    "all_transform_column = total_ppl | within_county | within_state | diff_state\n",
    "\n",
    "input_path =  './Data/Migration/'\n",
    "output_path = './Output/Migration/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross walk for cbsa and zcta\n",
    "walk = pd.read_csv('./Usage/us_xwalk.csv.gz')\n",
    "walk = walk.drop(columns=['tabblk2020']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_migration_data(file):\n",
    "    '''\n",
    "    input:\n",
    "        str file: filename in ./data folder\n",
    "    output:\n",
    "        csv files: migration + population data at ZCTA\n",
    "    '''\n",
    "\n",
    "    # read raw data and pre-precess\n",
    "    df = pd.read_csv(input_path + file,dtype='O', compression='gzip')\n",
    "    df = df.iloc[1: , :]\n",
    "    df['GEO_ID'] = df['GEO_ID'].str.slice(start=-5)\n",
    "\n",
    "    # merge cross-walk and raw data\n",
    "    all_needed_data = df[[col for col in df.columns if col in all_column.keys() or col == 'GEO_ID']].copy().rename(columns=all_column).replace('-', pd.NA).replace('**', pd.NA).fillna('0').astype(float)\n",
    "    all_needed_data = pd.merge(all_needed_data, walk, how='inner', left_on='GEO_ID', right_on='zcta')[['GEO_ID', 'cbsa'] + list(all_column.values())]\n",
    "\n",
    "    # create transform dataset\n",
    "    cities_migration = dict()\n",
    "\n",
    "    for key, val in all_transform_column.items():\n",
    "        cities_migration[key] = all_needed_data[val].sum(axis=1)\n",
    "\n",
    "    for key in transform_col.keys():\n",
    "        cities_migration['TM_' + key] = cities_migration['SC_' + key] + cities_migration['SS_' + key] + cities_migration['DS_' + key]\n",
    "        cities_migration['SCP_' + key] = cities_migration['SC_' + key]/cities_migration['TM_' + key].values\n",
    "    \n",
    "    cities_migration['cbsa'] = all_needed_data['cbsa']\n",
    "    cities_migration['GEO_ID'] = all_needed_data['GEO_ID']\n",
    "\n",
    "    cities_migration = pd.DataFrame.from_dict(cities_migration)\n",
    "    \n",
    "    pd.DataFrame.from_dict(cities_migration).to_csv(f'{output_path}{file[:-7]}-ZCTA.csv.gz', compression='gzip', sep = \",\", header=True, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all migration data\n",
    "for file in os.listdir(input_path):\n",
    "    transform_migration_data(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all output file to single historical data file\n",
    "migration_historical = []\n",
    "all_zipcode = set(range(0, 100000))\n",
    "start = 10000\n",
    "end = 0\n",
    "\n",
    "# merge all ZCTA that has data available from 2015 to 2022\n",
    "for file in os.listdir(output_path):\n",
    "    year = int(file[7:11])\n",
    "    start = min(year, start) \n",
    "    end = max(year, end)\n",
    "    migration_by_year = pd.read_csv(output_path + file, compression='gzip')\n",
    "    migration_by_year['year'] = year\n",
    "    all_zipcode = all_zipcode.intersection(migration_by_year.GEO_ID)\n",
    "    migration_historical.append(migration_by_year)\n",
    "\n",
    "all_zipcode = sorted(all_zipcode)\n",
    "\n",
    "migration_historical = [migration_by_year.set_index('GEO_ID').loc[all_zipcode].reset_index().sort_values('GEO_ID') for migration_by_year in migration_historical]\n",
    "\n",
    "# calculate the annual growth for total population (tt) and total migration rate (tm)\n",
    "tt = [migration_by_year[[col for col in migration_by_year.columns if 'TT_' in col]] for migration_by_year in migration_historical]\n",
    "tm = [migration_by_year[[col for col in migration_by_year.columns if 'TM_' in col]] for migration_by_year in migration_historical]\n",
    "\n",
    "population_change = []\n",
    "\n",
    "for idx, (prev_tt, curr_tt, prev_tm, curr_tm) in enumerate(zip(tt[:-1], tt[1:], tm[:-1], tm[1:]), start=1):\n",
    "    change = curr_tt - prev_tt.values\n",
    "    migration_historical[idx][[col for col in migration_by_year.columns if 'TT_' in col]] = change\n",
    "    change = curr_tm - prev_tm.values\n",
    "    migration_historical[idx][[col for col in migration_by_year.columns if 'TM_' in col]] = change\n",
    "   \n",
    "migration_historical.pop(0)\n",
    "\n",
    "all_tt_col = [col for col in migration_historical[0].columns if col not in ['year', 'cbsa', 'GEO_ID']]\n",
    "\n",
    "migration_historical_with_lag = []\n",
    "\n",
    "for idx in range(4, len(migration_historical)):\n",
    "    migration_current = migration_historical[idx]\n",
    "    for lag in range(1, 3):\n",
    "        lagging = migration_historical[idx-lag].rename(columns = {col: col + f'_LAG_{lag}' for col in all_tt_col}).drop(columns=['year'])\n",
    "        migration_current = pd.merge(migration_current, lagging)\n",
    "    migration_historical_with_lag.append(migration_current)\n",
    "    \n",
    "migration_historical_with_lag = pd.concat(migration_historical_with_lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data for cities in top 15\n",
    "top15_cities = {\n",
    "    'New York-Newark-Jersey City, NY-NJ-PA': 35620, \n",
    "    'Los Angeles-Long Beach-Anaheim, CA': 31080,\n",
    "    'Chicago-Naperville-Elgin, IL-IN-WI': 16980,\n",
    "    'Dallas-Fort Worth-Arlington, TX': 19100,\n",
    "    'Houston-The Woodlands-Sugar Land, TX': 26420,\n",
    "    'Washington-Arlington-Alexandria, DC-VA-MD-WV': 47900,\n",
    "    'Miami-Fort Lauderdale-Pompano Beach, FL': 33100,\n",
    "    'Philadelphia-Camden-Wilmington, PA-NJ-DE-MD': 37980,\n",
    "    'Atlanta-Sandy Springs-Alpharetta, GA': 12060,\n",
    "    'Phoenix-Mesa-Chandler, AZ': 38060,\n",
    "    'Boston-Cambridge-Newton, MA-NH': 14460,\n",
    "    'San Francisco-Oakland-Berkeley, CA': 41860,\n",
    "    'Riverside-San Bernardino-Ontario, CA': 40140,\n",
    "    'Detroit-Warren-Dearborn, MI': 19820,\n",
    "    'Seattle-Tacoma-Bellevue, WA': 42660\n",
    "    }\n",
    "\n",
    "migration_historical_top15 = migration_historical_with_lag.set_index('cbsa').loc[top15_cities.values()].reset_index()[['year', 'cbsa', 'GEO_ID'] + [col for col in migration_historical_with_lag.columns if 'TT' in col or 'TM' in col]].copy()\n",
    "\n",
    "migration_historical_top15 = migration_historical_top15[migration_historical_top15['year'] > 2015].set_index('GEO_ID').replace(np.inf, np.NaN)\n",
    "\n",
    "for col in migration_historical_top15.columns:\n",
    "    if col != 'year' or col != 'cbsa':\n",
    "        migration_historical_top15[col] = migration_historical_top15[col].fillna(migration_historical_top15.groupby(['year', 'cbsa'])[col].transform('mean'))\n",
    "\n",
    "migration_historical_top15.to_csv('./Output/final/MIGRATION_CHANGE_TOP15.csv.gz', compression='gzip', sep = \",\", header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbd_info = pd.read_csv('./Usage/zori_panel_zips.csv')[['zip', 'dist_to_cbd']]\n",
    "cbd_close = cbd_info.loc[cbd_info['dist_to_cbd'] < 7000]['zip']\n",
    "migration_historical_cbd = migration_historical_with_lag.set_index('GEO_ID').loc[cbd_close][['year', 'cbsa'] + [col for col in migration_historical_with_lag.columns if 'SCP' in col]]\n",
    "\n",
    "for col in migration_historical_cbd.columns:\n",
    "    if col != 'year' or col != 'cbsa':\n",
    "        migration_historical_cbd[col] = migration_historical_cbd[col].fillna(migration_historical_cbd.groupby(['year', 'cbsa'])[col].transform('mean'))\n",
    "\n",
    "migration_historical_cbd.to_csv('./Output/final/MIGRATION_CBD.csv.gz', compression='gzip', sep = \",\", header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_historical = []\n",
    "all_zipcode = set(range(0, 100000))\n",
    "start = 10000\n",
    "end = 0\n",
    "\n",
    "for file in os.listdir(output_path):\n",
    "    year = int(file[7:11])\n",
    "    start = min(year, start) \n",
    "    end = max(year, end)\n",
    "    migration_by_year = pd.read_csv(output_path + file, compression='gzip')\n",
    "    migration_by_year['year'] = year\n",
    "    all_zipcode = all_zipcode.intersection(migration_by_year.GEO_ID)\n",
    "    migration_historical.append(migration_by_year)\n",
    "\n",
    "all_zipcode = sorted(all_zipcode)\n",
    "migration_historical = [migration_by_year.set_index('GEO_ID').loc[all_zipcode].reset_index().sort_values('GEO_ID') for migration_by_year in migration_historical]\n",
    "\n",
    "migration_historical = [migration_by_year.set_index('cbsa').loc[top15_cities.values()].reset_index() for migration_by_year in migration_historical]\n",
    "\n",
    "all_ppl_col = [col for col in migration_historical[0].columns if col not in ['year', 'cbsa', 'GEO_ID']]\n",
    "\n",
    "migration_historical_with_lag = []\n",
    "\n",
    "for idx in range(4, len(migration_historical)):\n",
    "    migration_current = migration_historical[idx]\n",
    "    for lag in range(1, 3):\n",
    "        lagging = migration_historical[idx-lag].rename(columns = {col: col + f'_LAG_{lag}' for col in all_ppl_col}).drop(columns=['year'])\n",
    "        migration_current = pd.merge(migration_current, lagging)\n",
    "    migration_historical_with_lag.append(migration_current)\n",
    "\n",
    "migration_historical_top15 = pd.concat(migration_historical_with_lag)\n",
    "migration_historical_top15.to_csv('./Output/final/MIGRATION_TOTAL_TOP15.csv.gz', compression='gzip', sep = \",\", header=True, encoding='utf-8-sig', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psyc510",
   "language": "python",
   "name": "psyc510"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
